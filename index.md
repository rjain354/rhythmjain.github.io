## Hi there!
<table border="0">
 <tr>
    <td><b style="font-size:30px"></b></td>
    <td><b style="font-size:30px"></b></td>
 </tr>
 <tr>
    <td><img id="profile" align="centre" width="2500" alt="rhythmProfile" src="https://user-images.githubusercontent.com/78115400/151638487-2b039251-bbd4-4155-a3b6-1b3c37c4a165.jpg"></td>
    <td>I am a second year Master's student researcher at <a href="https://music.gatech.edu/master-science-music-technology">Georgia Tech Centre for Music Technology</a> in computational musicology, specializing in cross cultural music perception and music informatics. I have also done extensive work in human subject studies, data analysis, music information retrieval (MIR), digital signal processing, Max/MSP/Jitter, and symbolic music analysis of Hindustani classical music. Throughout the program I have worked on numerous projects some of which are highlighted below. I completed my under graduate studies from BITS Pilani, studying Mathematics and Computer Science. Shortly after I interned at various places including Amazon Development Centre, Goodera and IBM and worked as Software Engineer at Xilinx. I am currently an Summer Scientist intern at Pandora.</td>
 </tr>
</table>

<p>
<style>
  #profile{
    border-radius: 50%;
  }
  </style>

  </p>

## My Research 
### A Cross-cultural Examination of Raga perception and time-of-day
In Hindustani classical music (HCM), there is a historic association between a raga and the time of day or <i>prahar</i> that it should be performed at. We wanted to understand if <i>prahar</i> is purely a learned cultural aspect of the performance tradition of HCM, or if it can be perceived. In addition, we wished to separate the effects of enculturation as well as musical training. There have been several studies investigating the relationship between ragas and their evoked emotions. However, although prahar is related to emotion conveyance (i.e., performing the raga within the prahar is believed to maximize its emotional impact), we were specifically interested in whether the musical excerpts would be associated with a particular time of day.

More details regarding the background and analysis of this research are available at <a href="https://github.com/rhythmjain/TimeOfDay_PerceptionStudy">this GitHub repository</a>. This research has been accepted as a talk for <a href="[https://pheedloop.com/SMPC2022/site/](https://pheedloop.com/SMPC2022/site/sessions/?id=SES0SPY98F8HVWH80)">SMPC 2022</a> in Portland, Oregon.
## My Projects
### Sur Tarang
#### An interactive bio-feedback music system
<p>
<img align="centre" width="250" alt="Screen Shot 2022-01-28 at 19 00 47" src="https://user-images.githubusercontent.com/78115400/151637279-5af2591c-41aa-4b99-bb5c-c85eb44a932c.png"><img align="centre" width="492" alt="Screen Shot 2022-01-28 at 19 05 52" src="https://user-images.githubusercontent.com/78115400/151637624-ee30e406-d263-4a2b-a15d-da0acaa2de04.png">
</p>

Sur Tarang is an interactive and generative music system enabling musicians to control several parameters of the accompanying instruments without the need to touch or manipulate physical interfaces. Sur Tarang translated body and mind signals (bio signals via EEG) as well as automatic gesture recognition into
the sounds of the accompanying instruments. The sound design was done in Ableton Live and the generative component used dynamic time warping to recognize gestures and mirror the ”Jugalbandi” style of performance prominent in Hindustani classical music. For more details check out the demo(https://www.youtube.com/watch?v=88lU0gK0yoM) and the [github](https://github.com/rjain354/SurTarang) repository.

### Melograph
![Image](https://user-images.githubusercontent.com/78115400/151636271-d8e6794f-9df5-44bd-8b0d-94c9079dc83a.png) <img width="500" alt="Screen Shot 2022-01-28 at 18 43 55" src="https://user-images.githubusercontent.com/78115400/151636285-5e37ea06-171a-4dbd-8b8b-3990a0f6889a.png">

![image](https://user-images.githubusercontent.com/78115400/152262124-167cd5ec-3d3a-4027-b9de-382079915a2a.png)

MeloGraph is a visualization tool to aid musicians with their improvisation techniques. By extracting note-level pitch information from a given audio signal of a monophonic performance, this tool shows the relative note and transition prominence in the user’s performance. We used Harmonic Percussive Source Separation (HPSS) for onset detection, Normalized cross corelation function (NCCF) algorithms for F0 detection and NetworkX and Cytoscape libraries for visualization. For more details check out the [github](https://github.com/nol-alb/melograph_submission) repository.

### Software Synthesizer
We designed a python-based synthesizer that can be run using the command line. The codebase for the project can be found at this [github](https://github.com/rjain354/music6202/tree/main/FinalProject) repository. Taking symbolic music notation (Kern files) as input, the system produces synthesized output files in .wav format, downloaded to the user’s local machine. Synthesis of the signals was done using additive and wave table techniques.

### Others

#### Computational and Cognitive Lab Website
Over summer 2021, I designed a [website](https://ccml.gtcmt.gatech.edu/) for the Computational and Cognitive (CCM) lab at Georgia Tech which contains information about the latest news, team members and publications of the lab. This website allows the CCM lab to reach interested researchers seeking collaborations with the lab.

### Resume
Check out my presence on social media links and my [resume](https://github.com/rjain354/rjain354.github.io/files/7962616/Rhythm_Jain_2022.pdf).


